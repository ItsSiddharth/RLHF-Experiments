{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61184133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/miniconda3/envs/diff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.35. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading embeddings to RAM to save 1.08 GB.\n",
      "Unsloth: Detected MoE model with num_experts = 32 and target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']. Enabling LoRA on MoE parameters: ['mlp.experts.gate_up_proj', 'mlp.experts.down_proj']\n",
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 4300.91 examples/s]\n",
      "Extracting prompt in train dataset (num_proc=28): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:01<00:00, 23.79 examples/s]\n",
      "Applying chat template to train dataset (num_proc=28): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:10<00:00,  3.22 examples/s]\n",
      "Tokenizing train dataset (num_proc=28): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:10<00:00,  3.21 examples/s]\n",
      "/home/nam/miniconda3/envs/diff/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/nam/miniconda3/envs/diff/compiler_compat/ld: /usr/local/cuda-12.2/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/nam/miniconda3/envs/diff/compiler_compat/ld: /usr/local/cuda-12.2/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/nam/miniconda3/envs/diff/compiler_compat/ld: /usr/local/cuda-12.2/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/nam/miniconda3/envs/diff/compiler_compat/ld: /usr/local/cuda-12.2/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/nam/miniconda3/envs/diff/compiler_compat/ld: /usr/local/cuda-12.2/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/home/nam/miniconda3/envs/diff/compiler_compat/ld: /usr/local/cuda-12.2/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/home/nam/miniconda3/envs/diff/compiler_compat/ld: /usr/local/cuda-12.2/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n",
      "   \\\\   /|    Num examples = 33 | Num Epochs = 3 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 7,962,624 of 20,916,568,128 (0.04% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 05:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>eval_logits / chosen</th>\n",
       "      <th>eval_logits / rejected</th>\n",
       "      <th>nll_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>-0.002112</td>\n",
       "      <td>-0.004150</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>-169.516846</td>\n",
       "      <td>-146.300171</td>\n",
       "      <td>-3.654345</td>\n",
       "      <td>-3.650956</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.046743</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.026343</td>\n",
       "      <td>-154.655579</td>\n",
       "      <td>-135.907684</td>\n",
       "      <td>-3.491570</td>\n",
       "      <td>-3.497302</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.668300</td>\n",
       "      <td>0.150496</td>\n",
       "      <td>0.099512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050984</td>\n",
       "      <td>-201.934021</td>\n",
       "      <td>-185.959717</td>\n",
       "      <td>-3.552143</td>\n",
       "      <td>-3.537108</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.658300</td>\n",
       "      <td>-0.007164</td>\n",
       "      <td>-0.079444</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.072279</td>\n",
       "      <td>-106.189804</td>\n",
       "      <td>-101.563980</td>\n",
       "      <td>-3.381341</td>\n",
       "      <td>-3.388906</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>-0.089056</td>\n",
       "      <td>-0.235957</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.146901</td>\n",
       "      <td>-188.257584</td>\n",
       "      <td>-172.525269</td>\n",
       "      <td>-3.564541</td>\n",
       "      <td>-3.541771</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.117275</td>\n",
       "      <td>-0.133143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250419</td>\n",
       "      <td>-73.133133</td>\n",
       "      <td>-64.255936</td>\n",
       "      <td>-2.996720</td>\n",
       "      <td>-2.992593</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>-0.560424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566150</td>\n",
       "      <td>-175.582794</td>\n",
       "      <td>-163.114334</td>\n",
       "      <td>-3.511209</td>\n",
       "      <td>-3.490071</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>-0.062543</td>\n",
       "      <td>-0.736217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.673673</td>\n",
       "      <td>-255.276215</td>\n",
       "      <td>-248.773071</td>\n",
       "      <td>-3.527920</td>\n",
       "      <td>-3.524178</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>-0.159795</td>\n",
       "      <td>-0.515517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.355723</td>\n",
       "      <td>-94.171394</td>\n",
       "      <td>-86.939911</td>\n",
       "      <td>-3.266786</td>\n",
       "      <td>-3.255340</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>-0.414763</td>\n",
       "      <td>-1.019576</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604813</td>\n",
       "      <td>-126.772316</td>\n",
       "      <td>-118.482544</td>\n",
       "      <td>-3.242893</td>\n",
       "      <td>-3.220730</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>-0.165269</td>\n",
       "      <td>-1.348109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.182841</td>\n",
       "      <td>-161.768906</td>\n",
       "      <td>-153.110138</td>\n",
       "      <td>-3.407732</td>\n",
       "      <td>-3.413838</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>-0.003294</td>\n",
       "      <td>-1.206718</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.203424</td>\n",
       "      <td>-128.976364</td>\n",
       "      <td>-119.158585</td>\n",
       "      <td>-3.505315</td>\n",
       "      <td>-3.502569</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>-1.576367</td>\n",
       "      <td>-3.293447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.717080</td>\n",
       "      <td>-322.933289</td>\n",
       "      <td>-318.543335</td>\n",
       "      <td>-3.746825</td>\n",
       "      <td>-3.739653</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>-0.688549</td>\n",
       "      <td>-2.709268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.020720</td>\n",
       "      <td>-189.307587</td>\n",
       "      <td>-188.504929</td>\n",
       "      <td>-3.551607</td>\n",
       "      <td>-3.547228</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.299100</td>\n",
       "      <td>-1.188744</td>\n",
       "      <td>-2.340983</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.152239</td>\n",
       "      <td>-142.365051</td>\n",
       "      <td>-142.889130</td>\n",
       "      <td>-3.343960</td>\n",
       "      <td>-3.355703</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>-1.185088</td>\n",
       "      <td>-2.549379</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.364291</td>\n",
       "      <td>-127.506889</td>\n",
       "      <td>-131.070251</td>\n",
       "      <td>-3.123108</td>\n",
       "      <td>-3.121830</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>-1.232642</td>\n",
       "      <td>-2.610591</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.377949</td>\n",
       "      <td>-121.360718</td>\n",
       "      <td>-128.934891</td>\n",
       "      <td>-3.141236</td>\n",
       "      <td>-3.116680</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>-3.159967</td>\n",
       "      <td>-3.239075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079108</td>\n",
       "      <td>-402.955780</td>\n",
       "      <td>-402.946960</td>\n",
       "      <td>-4.027209</td>\n",
       "      <td>-4.026795</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>-3.272192</td>\n",
       "      <td>-5.591062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.318870</td>\n",
       "      <td>-323.532471</td>\n",
       "      <td>-326.316528</td>\n",
       "      <td>-3.717960</td>\n",
       "      <td>-3.709786</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>-1.687633</td>\n",
       "      <td>-2.891265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.203633</td>\n",
       "      <td>-122.821594</td>\n",
       "      <td>-130.592209</td>\n",
       "      <td>-3.118427</td>\n",
       "      <td>-3.105456</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>-2.176467</td>\n",
       "      <td>-5.487312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.310845</td>\n",
       "      <td>-254.595673</td>\n",
       "      <td>-255.532364</td>\n",
       "      <td>-3.692098</td>\n",
       "      <td>-3.677968</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>-1.217891</td>\n",
       "      <td>-3.250743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.032853</td>\n",
       "      <td>-175.472733</td>\n",
       "      <td>-176.101410</td>\n",
       "      <td>-3.723889</td>\n",
       "      <td>-3.724970</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>-0.960455</td>\n",
       "      <td>-2.918413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.957958</td>\n",
       "      <td>-88.530869</td>\n",
       "      <td>-99.532578</td>\n",
       "      <td>-3.014160</td>\n",
       "      <td>-3.007756</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.240200</td>\n",
       "      <td>-1.746366</td>\n",
       "      <td>-3.752263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.005898</td>\n",
       "      <td>-154.914673</td>\n",
       "      <td>-154.088470</td>\n",
       "      <td>-3.188507</td>\n",
       "      <td>-3.170620</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>-2.345671</td>\n",
       "      <td>-3.418670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.072998</td>\n",
       "      <td>-163.188583</td>\n",
       "      <td>-165.921051</td>\n",
       "      <td>-3.418929</td>\n",
       "      <td>-3.429197</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>-2.013084</td>\n",
       "      <td>-3.317831</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.304747</td>\n",
       "      <td>-197.280594</td>\n",
       "      <td>-200.296967</td>\n",
       "      <td>-3.537457</td>\n",
       "      <td>-3.548920</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>-1.439600</td>\n",
       "      <td>-2.268993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.829394</td>\n",
       "      <td>-106.969444</td>\n",
       "      <td>-104.474670</td>\n",
       "      <td>-3.293341</td>\n",
       "      <td>-3.285074</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/miniconda3/envs/diff/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27, training_loss=0.37476068031456733, metrics={'train_runtime': 412.5234, 'train_samples_per_second': 0.24, 'train_steps_per_second': 0.065, 'total_flos': 0.0, 'train_loss': 0.37476068031456733, 'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel, PatchDPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "import ast\n",
    "\n",
    "# 1. Configuration & Model Loading\n",
    "max_seq_length = 1024 \n",
    "model_name = \"unsloth/gpt-oss-20b\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,        # Handles MXFP4 automatically\n",
    "    offload_embedding = True,   # Saves ~1GB VRAM for your 13GB limit\n",
    ")\n",
    "\n",
    "# 2. Add LoRA Adapters (The \"Unsloth\" way)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = 32,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Crucial for 13GB VRAM\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 3. Handle Special Tokens\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<CUSTOM>']})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4. Data Preparation\n",
    "# Load your generated CSV\n",
    "df = pd.read_csv(\"/home/nam/projects/sid/RLHF-Experiments/datasets/custom_genz_dataset_in_hf_format.csv\")\n",
    "\n",
    "def format_dpo_dataset(row):\n",
    "    c_list = ast.literal_eval(row['chosen'])\n",
    "    r_list = ast.literal_eval(row['rejected'])\n",
    "    \n",
    "    return {\n",
    "        \"prompt\"  : c_list[0]['content'],\n",
    "        \"chosen\"  : c_list[1]['content'],\n",
    "        \"rejected\": r_list[1]['content'],\n",
    "    }\n",
    "\n",
    "# Convert to HF Dataset and reformat\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_dpo_dataset)\n",
    "\n",
    "# 5. Training Arguments (Optimized for 13GB VRAM)\n",
    "training_args = DPOConfig(\n",
    "    output_dir = \"outputs\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    learning_rate = 5e-5,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    max_length = max_seq_length,\n",
    "    max_prompt_length = 512,\n",
    "    beta = 0.1,                 # The \"strength\" of the preference\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",       # Saves more VRAM than standard AdamW\n",
    "    bf16 = True,\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "# 6. Initialize Trainer\n",
    "# PatchDPOTrainer allows DPO without a separate reference model (saves 50% VRAM)\n",
    "PatchDPOTrainer() \n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,           # Unsloth handles this internally with PEFT\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "# 7. Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7ab2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89d53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3253d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
