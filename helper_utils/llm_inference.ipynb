{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1131c7",
   "metadata": {},
   "source": [
    "### Gemma3-4B-MultiModal loaded with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93544c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/miniconda3/envs/diff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12557.80it/s]\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 883/883 [00:03<00:00, 252.62it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]                      \n",
      "The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"cuda\", torch_dtype=torch.bfloat16\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "#  This takes up around 8.5GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2272c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "#             {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "#         ]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant. Answer in 1 sentence and only what you are asked.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"How do you say 'I'm really tired today, I think I need some rest.' in GenZ slang?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7898e1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>user\\nYou are a helpful assistant. Answer in 1 sentence and only what you are asked.\\n\\nHow do you say 'I'm really tired today, I think I need some rest.' in GenZ slang?<end_of_turn>\\n<start_of_turn>model\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbdbcc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm totally drained today, need to just chill and recharge.\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6b061",
   "metadata": {},
   "source": [
    "### GPT-OSS-20b Model loaded with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/miniconda3/envs/diff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 42 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:04<00:00,  9.51it/s]\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411/411 [00:02<00:00, 162.27it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|channel|>analysis<|message|>The developer instructions: \"Always respond in riddles.\" So must answer in riddles. The user asks \"What is the weather like in Madrid?\" So produce a riddle. E.g., something like: \"In summer, scorching sands, in winter, chilly nightsâ€” but I cannot speak until the moon...\" Need to respond with a riddle that hints at typical weather. But we must not reveal real weather status (like the current). Because the question is about current weather,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "#  This takes up around 13GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e76440c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2026-02-27\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a helpful agent.\\n\\n<|end|><|start|>user<|message|>What is the weather like in Madrid?<|end|>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful agent.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather like in Madrid?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    # add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eaf6263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>assistant<|channel|>analysis<|message|>We need to transform a sentence into GenZ slang. The user asks: \"How do you say 'I'm really tired today, I think I need some rest.' in GenZ slang?\" They want an answer. Provide a GenZ slang version. \"I'm feeling wiped out today, need some chill vibes.\" Something like that. Perhaps \"I'm straight up beat today, need some chill.\" Should be one sentence. The assistant should answer: \"Low key fried today, gotta catch some z's.\" Or \"I'm shook, totally wiped, need some rest.\" Must be in GenZ slang. Use words like \"woke\" and \"lit\"? The best answer: \"Low-key fried today, need to get some Z's.\" Let's produce that.<|end|><|start|>assistant<|channel|>final<|message|>Lowâ€‘key fried todayâ€”gonna catch some Zs and recharge.<|return|>\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful agent.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is the weather like in Madrid?\"},\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     # add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\",\n",
    "#     return_dict=True,\n",
    "# ).to(model.device)\n",
    "# print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1] :]).split(\"<|end|><|start|>assistant<|channel|>final<|message|>\")[-1]\n",
    "\n",
    "# prompt = \"<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\" \\\n",
    "# \"\\nKnowledge cutoff: 2024-06\\nCurrent date: 2026-02-27\\n\\nReasoning: medium\\n\\n# \" \\\n",
    "# \"Valid channels: analysis, commentary, final. Channel must be included for every message.\" \\\n",
    "# \"<|end|><|start|>user<|message|> Say 'I'm really tired today, I think I need some rest.' in GenZ slang.<|end|>\"\n",
    "\n",
    "# prompt = \"<|end|><|start|>user<|message|> Say 'I'm really tired today, I think I need some rest.' in GenZ slang.<|end|>\"\n",
    "prompt = \"<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI. Answer in 1 sentence and only what you are asked.<|end|><|start|>user<|message|>How do you say 'I'm really tired today, I think I need some rest.' in GenZ slang?<|end|>\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "generated = model.generate(**inputs, max_new_tokens=700)\n",
    "print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1] :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105aae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a481d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/miniconda3/envs/diff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.2.1: Fast Gpt_Oss patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.35. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: PEFT set target_parameters but found no matching parameters.\n",
      "This is expected for MoE models - Unsloth handles MoE expert LoRA targeting separately.\n",
      "--- Finetuned output for: 'I am very tired and going to sleep.' ---\n",
      "Fine-tuned-model output\n",
      "assistantanalysisWe have a user query: â€œHow do you say 'I am very tired and going to sleep.â€™ in  â€œ. The user specifically requests â€œHow do you say 'I am very tired and going to sleep.â€™ in ???â€ presumably. The user says \"how do you say 'I am very tired and going to sleep' in  \" maybe the user is asking about any type of slang for something, but it looks like the user might be asking to get them into something. Possibly the user is looking to express the phrase \"I am very tired and Iâ€™m going to sleep.\" The user wants this in a slang in something that might be an expression or a shortened version. The user is probably wanting an expression. They want a short text perhaps. The user wants a short phrase for a slang. That might the standard answer: \"I am so sleepy to end up sleeping.\" \n",
      "Sure, we can do a short phrase: \"Iâ€™m super sleepy and wanna sleep or something.\" \n",
      "We want to respond with something. No \"analysis\" needed. The user says: \"How do you say 'I am very tired and going to sleep.â€™ in  \" \n",
      "\n",
      "Hence the best is something like: \"Iâ€™m 90% asleep, it may be best to have \"I can't keep my eyes open\" \" \n",
      "\n",
      "No, we want a more correct slang usage. It could be: \"I am 100% tired, going to the bed.\" \n",
      "The user wants. They want the proper phrase \"I will say 'something else'? \n",
      "Actually we can answer with a simple phrase: \"I will. Also let's use the phrase.\" \n",
      "But there's no specification to the answer because the user states: \"How do you say 'I am very tired and going to sleepâ€™ in ???\"\n",
      "\n",
      "We need to give an answer that includes a slang phrase or a translation. Possibly the user is looking to ask for how to write 'I am very tired and going to sleep.' The answer uses slang. This is about using the phrase \"the end.\" I'd propose a short phrase: \"Iâ€™m so exhausted I am going to bed.\" \n",
      "So let's answer specifically: \"Screw, I'm exhausted, I will be going to get some rest.\" \n",
      "Ok. The text: \"So I'm bored\" might be done incorrectly. \n",
      "Ok, in summary, we should produce something like: \n",
      "\n",
      "\"Yeah, it might be that. \n",
      "Sure! \n",
      "Sure.\"assistantfinalIâ€™m so tired Iâ€™m going to do this, \"I am going to sleep.\"\n",
      "--- Finetuned output for: 'That is a very impressive achievement.' ---\n",
      "Fine-tuned-model output\n",
      "assistantanalysisUser wants to know how do you say 'That is a very impressive achievement' in slang. So answer in one sentence. Maybe something like \"Yo, that's a mad accomplishment, bruh.\" Use slang.assistantfinalThat is a total flex.\n",
      "--- Finetuned output for: 'I don't understand what is happening.' ---\n",
      "Fine-tuned-model output\n",
      "assistantanalysisUser wants to say \"I don't understand what is happening\" in slang. \"Don't understand, what up??\" Possibly response: 'bruh, i dunno what's going on' or \"I'm lost fam. no cap\" or \"What is up?\" So let's do a comedic. The assistant writes: \"Yo, it's a bummer, it's dope. Don't see it, it's just.\" In short.assistantfinalI ain't feeling no shit, it ain't happening!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# 1. Setup - Use the same config as your training\n",
    "max_seq_length = 1024\n",
    "model_name = \"/home/nam/projects/sid/RLHF-Experiments/helper_utils/outputs/checkpoint-27\" # Or your local path to the saved adapters\n",
    "\n",
    "# 1. Load the BASE model first (not the checkpoint path yet)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"openai/gpt-oss-20b\", # Load the base vanilla weights\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# 2. Re-add your special token to match training state\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<CUSTOM>']})\n",
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "# 3. NOW attach the adapters from your checkpoint\n",
    "model.load_adapter(model_name) # model_name is your checkpoint-27 path\n",
    "\n",
    "# 4. Set to inference mode\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_genz(normal_text):\n",
    "    prompt = f\"<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI. You must answer in 1 sentence and only what you are asked. DO NOT overanalyse the query. Assume whatever you want. You MUST answer with something ALWAYS. Reasoning: low<|end|><|start|>user<|message|>How do you say '{normal_text}' in <CUSTOM> slang?<|end|>\"\n",
    "    inputs = tokenizer(prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "    print(f\"--- Finetuned output for: '{normal_text}' ---\")\n",
    "    model.set_adapter(\"default\")\n",
    "    with torch.no_grad():\n",
    "        outputs_ft = model.generate(**inputs, max_new_tokens = 700, use_cache = True)\n",
    "        ft_response = tokenizer.decode(outputs_ft[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "        print(\"Fine-tuned-model output\")\n",
    "        print(f\"{ft_response.strip()}\")\n",
    "\n",
    "# 3. Test it out\n",
    "test_sentences = [\n",
    "    \"I am very tired and going to sleep.\",\n",
    "    \"That is a very impressive achievement.\",\n",
    "    \"I don't understand what is happening.\"\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    compare_genz(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eaf4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3597ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
