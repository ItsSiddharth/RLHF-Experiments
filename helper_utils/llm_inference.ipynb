{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1131c7",
   "metadata": {},
   "source": [
    "### Gemma3-4B-MultiModal loaded with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93544c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/miniconda3/envs/diff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 12557.80it/s]\n",
      "Loading weights: 100%|██████████| 883/883 [00:03<00:00, 252.62it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]                      \n",
      "The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"cuda\", torch_dtype=torch.bfloat16\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "#  This takes up around 8.5GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2272c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "#             {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "#         ]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant. Answer in 1 sentence and only what you are asked.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"How do you say 'I'm really tired today, I think I need some rest.' in GenZ slang?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7898e1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>user\\nYou are a helpful assistant. Answer in 1 sentence and only what you are asked.\\n\\nHow do you say 'I'm really tired today, I think I need some rest.' in GenZ slang?<end_of_turn>\\n<start_of_turn>model\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbdbcc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm totally drained today, need to just chill and recharge.\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6b061",
   "metadata": {},
   "source": [
    "### GPT-OSS-20b Model loaded with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/miniconda3/envs/diff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 42 files: 100%|██████████| 42/42 [00:04<00:00,  9.51it/s]\n",
      "Loading weights: 100%|██████████| 411/411 [00:02<00:00, 162.27it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|channel|>analysis<|message|>The developer instructions: \"Always respond in riddles.\" So must answer in riddles. The user asks \"What is the weather like in Madrid?\" So produce a riddle. E.g., something like: \"In summer, scorching sands, in winter, chilly nights— but I cannot speak until the moon...\" Need to respond with a riddle that hints at typical weather. But we must not reveal real weather status (like the current). Because the question is about current weather,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "#  This takes up around 13GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e76440c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2026-02-27\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\\n\\nYou are a helpful agent.\\n\\n<|end|><|start|>user<|message|>What is the weather like in Madrid?<|end|>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful agent.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather like in Madrid?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    # add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eaf6263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>assistant<|channel|>analysis<|message|>We need to transform a sentence into GenZ slang. The user asks: \"How do you say 'I'm really tired today, I think I need some rest.' in GenZ slang?\" They want an answer. Provide a GenZ slang version. \"I'm feeling wiped out today, need some chill vibes.\" Something like that. Perhaps \"I'm straight up beat today, need some chill.\" Should be one sentence. The assistant should answer: \"Low key fried today, gotta catch some z's.\" Or \"I'm shook, totally wiped, need some rest.\" Must be in GenZ slang. Use words like \"woke\" and \"lit\"? The best answer: \"Low-key fried today, need to get some Z's.\" Let's produce that.<|end|><|start|>assistant<|channel|>final<|message|>Low‑key fried today—gonna catch some Zs and recharge.<|return|>\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful agent.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is the weather like in Madrid?\"},\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     # add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\",\n",
    "#     return_dict=True,\n",
    "# ).to(model.device)\n",
    "# print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1] :]).split(\"<|end|><|start|>assistant<|channel|>final<|message|>\")[-1]\n",
    "\n",
    "# prompt = \"<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\" \\\n",
    "# \"\\nKnowledge cutoff: 2024-06\\nCurrent date: 2026-02-27\\n\\nReasoning: medium\\n\\n# \" \\\n",
    "# \"Valid channels: analysis, commentary, final. Channel must be included for every message.\" \\\n",
    "# \"<|end|><|start|>user<|message|> Say 'I'm really tired today, I think I need some rest.' in GenZ slang.<|end|>\"\n",
    "\n",
    "# prompt = \"<|end|><|start|>user<|message|> Say 'I'm really tired today, I think I need some rest.' in GenZ slang.<|end|>\"\n",
    "prompt = \"<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI. Answer in 1 sentence and only what you are asked.<|end|><|start|>user<|message|>How do you say 'I'm really tired today, I think I need some rest.' in GenZ slang?<|end|>\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "generated = model.generate(**inputs, max_new_tokens=700)\n",
    "print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1] :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105aae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a481d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c57a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eaf4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3597ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
